# 机器学习算法
机器学习算法可以分为监督学习、无监督学习、强化学习等类别。

## 监督学习算法：

- 线性回归（Linear Regression）：用于回归任务，预测连续的数值。
- 逻辑回归（Logistic Regression）：用于二分类任务，预测类别。
- 支持向量机（SVM）：用于分类任务，构建超平面进行分类。
- 决策树（Decision Tree）：基于树状结构进行决策的分类或回归方法。

## 无监督学习算法：

- K-means 聚类：通过聚类中心将数据分组。
- 主成分分析（PCA）：用于降维，提取数据的主成分。

## 线性回归（Linear Regression）
线性回归是一种用于回归问题的算法，它通过学习输入特征与目标值之间的线性关系，来预测一个连续的输出。

应用场景：预测房价、股票价格等。

**线性回归的目标是找到一个最佳的线性方程**：
$$
y=\omega_1x_1 + \omega_2x_2 + ...+\omega_nx_n + b
$$

- y 是预测值（目标值）。
- $x_1$，$x_2$，$x_n$ 是输入特征。
- $\omega_1$，$\omega_2$，$\omega_n$是待学习的权重（模型参数）。
- b 是偏置项。

误差函数是均方误差
$$
MSE=\frac{1}{n}\sum(y_i-y_{pred,i})^2
$$

## 逻辑回归（Logistic Regression）
逻辑回归是一种用于分类问题的算法，尽管名字中包含"回归"，它是用来处理**二分类问题**的。

逻辑回归通过学习输入特征与类别之间的关系，来预测一个类别标签。

应用场景：垃圾邮件分类、疾病诊断（是否患病）。

逻辑回归的输出是一个概率值，表示样本属于某一类别的概率。
通常使用sigmoid函数：
$$
P(y=1|X)=\sigma(w^TX+b)\\
\sigma(z)= \frac{1}{1+e^{-z}}

$$
将输出映射到0-1之间

损失函数是对数损失函数(Log Loss)
$$
J(w,b) = -\frac{1}{m}\sum_{i = 1}^m[y^{(i)}log(h_\theta(x^{i})) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))]
$$

## 支持向量机（SVM）
支持向量机是一种常用的分类算法，它通过构造超平面来最大化类别之间的间隔（Margin），使得分类的误差最小。

应用场景：文本分类、人脸识别等。


## 决策树（Decision Tree）
决策树是一种基于树结构进行决策的分类和回归方法。它通过一系列的"判断条件"来决定一个样本属于哪个类别。

应用场景：客户分类、信用评分等。

决策树通过树状结构来表示决策过程，每个内部节点代表一个特征或属性的测试，每个分支代表测试的结果，每个叶节点代表一个类别或值。

**决策树的基本概念**：

- 节点（Node）：树中的每个点称为节点。根节点是树的起点，内部节点是决策点，叶节点是最终的决策结果。
- 分支（Branch）：从一个节点到另一个节点的路径称为分支。
- 分裂（Split）：根据某个特征将数据集分成多个子集的过程。
- 纯度（Purity）：衡量一个子集中样本的类别是否一致。纯度越高，说明子集中的样本越相似。

**决策树的工作原理**：
决策树通过递归地将数据集分割成更小的子集来构建树结构。具体步骤如下：

- 选择最佳特征：根据某种标准（如信息增益、基尼指数等）选择最佳特征进行分割。
- 分割数据集：根据选定的特征将数据集分成多个子集。
- 递归构建子树：对每个子集重复上述过程，直到满足停止条件（如所有样本属于同一类别、达到最大深度等）。
- 生成叶节点：当满足停止条件时，生成叶节点并赋予类别或值。